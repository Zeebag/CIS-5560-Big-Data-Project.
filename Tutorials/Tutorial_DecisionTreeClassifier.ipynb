{"cells":[{"cell_type":"markdown","source":["<a href=\"http://www.calstatela.edu/centers/hipic\"><img align=\"left\" src=\"https://avatars2.githubusercontent.com/u/4156894?v=3&s=100\"><image/>\n</a>\n<img align=\"right\" alt=\"California State University, Los Angeles\" src=\"http://www.calstatela.edu/sites/default/files/groups/California%20State%20University%2C%20Los%20Angeles/master_logo_full_color_horizontal_centered.svg\" style=\"width: 360px;\"/>\n\n# CIS5560 Term Project Tutorial"],"metadata":{}},{"cell_type":"markdown","source":["------\n#### Authors: [Nikita Marathe](https://www.linkedin.com/in/nikita-dhanraj-marathe-544323138); [Nikeeta Akbari](https://www.linkedin.com/in/nikeeta-akbari/); [Rohit Tiwari](https://www.linkedin.com/in/rohit-tiwari-626012102/); [Zeeshan Khan](https://www.linkedin.com/in/zeeshan-k-0a6330b4/)\n\n#### Instructor: [Jongwook Woo](https://www.linkedin.com/in/jongwook-woo-7081a85)\n\n#### Date: 05/18/2018"],"metadata":{}},{"cell_type":"markdown","source":["## Home Mortgage Prediction Model using Desicion Tree Classifier"],"metadata":{}},{"cell_type":"markdown","source":["In this exercise, you will implement Two-Class Logistic Regression model to predict the status of the Home Mortgage Application.\n\n###Pre-requisites:\n\n1. A Spark cluster, with default configuration as part of Databricks community edition.\n2. Dataset for Home Mortgage Disclosure Act. Available to download here: https://www.consumerfinance.gov/data-research/hmda/explore\n3. Databricks community edition account. Signup for free here : https://databricks.com/try-databricks\n\n###Creating a Cluster\n\nSign into your databricks account and go to Clusters option on the left and click on create cluster. Give the cluster name and click create cluster."],"metadata":{}},{"cell_type":"markdown","source":["###Overview\n\nYou should follow the steps below to build, train and test the model from the source data:\n\n1. Import the HMDA_Data.csv as table in databricks.\n2. Change the datatype for all the columns as required.\n3. Preprocess the data by removing missing values.\n4. Prepare the data with the features (input columns, output column as label)\n5. Split the data using data.randomSplit(): Training and Testing;Rename label to trueLabel in test.\n6. Transform the columns to a vector using VectorAssembler\n7. Set features and label from the vector\n8. Build a Decision Tree Classifier Model with the label and features\n9. Build a Pipeline with 2 stages, VectorAssembler and Decision Tree Classifier.\n10. Train the model\n11. Prepare the testing Data Frame with features and label from the vector.\n12. Predict and test the testing Data Frame using the model trained at the step 10.\n14. Assess the performance of the model using AUC, precision and recall."],"metadata":{}},{"cell_type":"markdown","source":["###Upload CSV as a Table\n\nThe data for this project is provided as a CSV file containing details of Home Mortgage Application status. The data includes specific characteristics (or features) for each application, as well as a label column indicating the status of the application."],"metadata":{}},{"cell_type":"markdown","source":["### Prepare the Data\n\nFirst, import the libraries you will need and prepare the training and test data:"],"metadata":{}},{"cell_type":"code","source":["# Import Spark SQL and Spark ML libraries\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n"],"metadata":{"collapsed":false,"scrolled":false},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Read your csv file from its table at Databricks"],"metadata":{}},{"cell_type":"code","source":["# Load the source data\ncsv = sqlContext.sql(\"SELECT * FROM final_csv\")"],"metadata":{"collapsed":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Select Feature Columns and label Column\nUsing the SQL commands, 15 feature columns are selected which are highly important for our predicitve model, and one label column called action_taken_name is selected to perform predicition."],"metadata":{}},{"cell_type":"code","source":["# Select features and label\ndata1 = csv.select(\"tract_to_msamd_income\",\"population\", \"minority_population\", \"loan_amount_000s\", \"applicant_income_000s\",\"purchaser_type_name\",\"preapproval_name\",\"owner_occupancy_name\",\"loan_type_name\",\"lien_status_name\",\"co_applicant_sex_name\",\"co_applicant_race_name_1\",\"co_applicant_ethnicity_name\",\"agency_name\",\"agency_abbr\",col(\"action_taken_name\").alias(\"label\"))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Perform Data Transformation\nAfter selecting feature columns, it is important to trnasform our dataset by cleaning it. For this purpose, we have used dropna() function as this function drops any row which has null values."],"metadata":{}},{"cell_type":"code","source":["# Drop rows from the table even if one value is null\ndata2 = data1.dropna()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Splitting of Data\nWe have used randomSplit() funciton to split our data. We have used 70% of data for traning and 30% for testing. General less ratio of data is used for testing and more ratio is used training."],"metadata":{}},{"cell_type":"code","source":["# Split the data\nsplits = data2.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1].withColumnRenamed(\"label\", \"trueLabel\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### Define the Pipeline\nNow define a pipeline that two stages. First stage is to create a vector of feature columns and trains a classification model using the Logistic Regression Algorithm. The input parameters for the Logistic Regression Algorithm are label column (action_taken_name) and featureCol (vector created in stage 1). In the second stage pipeline is created using the assember and the logistic regression alogrithm."],"metadata":{}},{"cell_type":"code","source":["vectorAssembler = VectorAssembler(inputCols = [\"tract_to_msamd_income\",\"population\", \"minority_population\", \"loan_amount_000s\", \"applicant_income_000s\",\"purchaser_type_name\",\"preapproval_name\",\"owner_occupancy_name\",\"loan_type_name\",\"lien_status_name\",\"co_applicant_sex_name\",\"co_applicant_race_name_1\",\"co_applicant_ethnicity_name\",\"agency_name\",\"agency_abbr\"], outputCol=\"features\")\n#Model1 - Decision Tree \ndecision = DecisionTreeClassifier(labelCol=\"label\", featuresCol= \"features\")\npipeline = Pipeline(stages=[vectorAssembler, decision])"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Train the Model"],"metadata":{}},{"cell_type":"code","source":["# define list of models made from Train Validation Split and Cross Validation\nmodel = pipeline.fit(train)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Test the Model\nNow you're ready to apply the model to the test data."],"metadata":{}},{"cell_type":"code","source":["prediction = model.transform(test)\npredicted = prediction.select(\"prediction\", \"trueLabel\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Review the Area Under ROC\nAnother way to assess the performance of a classification model is to measure the area under a ROC curve for the model. the spark.ml library includes a **BinaryClassificationEvaluator** class that you can use to compute this."],"metadata":{}},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc = evaluator.evaluate(prediction)\nprint \"Average Accuracy =\", auc"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### Review the Recall And Precision\nAnother way to assess the performance of a classification model is to measure the precision and recall."],"metadata":{}},{"cell_type":"code","source":["tp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 1\").count())\nfp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 0\").count())\ntn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 0\").count())\nfn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 1\").count())\nmetrics = spark.createDataFrame([(\"Precision\", tp / (tp + fp)), (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\nmetrics.show()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["### Result shows:\n* AUC: 0.898\n* Precision: 0.982\n* Recall: 0.812"],"metadata":{}},{"cell_type":"markdown","source":["References:\n1. [Importing Tables in Databricks](https://docs.databricks.com/user-guide/tables.html)\n1. [Markdown Cells in Jupyter](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html)\n1. [Markdown Cheatshee](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)\n1. [Markdown Guide](https://help.ghost.org/hc/en-us/articles/224410728-Markdown-Guide)"],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 2 with Spark 2.0","language":"python","name":"python2-spark20"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.11","nbconvert_exporter":"python","file_extension":".py"},"name":"Midterm2 Python+Classfication+Parameter+Tuning","notebookId":2230895938021588},"nbformat":4,"nbformat_minor":0}
